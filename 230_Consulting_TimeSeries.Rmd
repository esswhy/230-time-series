---
title: "PSTAT 230: Time Series Consulting"
author: "Selin Karabulut, Sophia Arabadjis, Shuying Yu"
date: "4/27/2021"
output: 
  html_document:
    df_print: paged
    theme: flatly
    toc: true
    toc_float: true
    number_sections: true
    code_folding: hide
bibliography: refs_file.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


# An intuition for time series and data analysis in R


We are using an example data set used in the Appendix by @shumway_time_2017. Additional examples for code using `R` to explore time series data is also found in @hyndman_forecasting_nodate. 


Time series data can be thought of as data consistently measured or observed at different points in time. In this way, the data have an implicit _order_ or flow, and questions about time series data often involve questions about change over time. 

```{r, message=F, warning=F}
#Libraries
library(tidyverse)
library(lubridate)
library(PerformanceAnalytics)
library(Hmisc)
library(astsa)
library(ggfortify)

# Load data sets
data("cmort")
data("part")
cmort <- cmort
part <- part
paste("Particulate Matter Data", paste(head(part), collapse = " "))
paste("Cardiac Mortality Data", paste(head(cmort),collapse = " "))
```

As a running example, we can work with three data sets from Shumway and Stoffer's 2017 book @shumway_time_2017. 

(1) The first data set is called `cmort` and captures the average weekly cardiovascular mortality in Los Angeles County from 1970 to 1980.

(2) The second data set is called `part` and captures the average weekly particulate matter concentration in Los Angeles County from 1970 to 1980.

(3) The third data set is called `temp` and captures the average weekly temperature in Los Angeles County from 1970 to 1980. 

We can instruct the `R` to read and format each data set using `ts(data, start=(),frequency)` arguments, where in the `start` we dictate the year and week of the data start, and the frequency specifies how many weeks (52) to consider. Using `ts` objects is very handy because we don't have to store and manipulate dates (if they are regular interval), and visualization becomes easier.

```{r}
## Load in and save data from astsa package
data(cmort)
cmort <- cmort
data(tempr)
tempr <- tempr
data(part)
part <- part
## Save as ts objects
cmort.ts <- ts(cmort, start = c(1970,1),frequency = 52)
cmort.ts
part.ts <- ts(part, start = c(1970,1), frequency = 52)
part.ts
tempr.ts <- ts(tempr, start=c(1970,1), frequency = 52)
tempr.ts
```

<br>

The first thing we might want to do is simply visualize the data. Because the `cmort`,`tempr` and `part` time series are all on the same time scale (weekly, 1970-1980), we can plot them on the same plot.

```{r, df_print}
#Set figure parameters
layout(matrix(c(1:3, 1:3), ncol=2), height=c(.5,.5, .5)) 
par(mar=c(.2,2,0,.5), oma=c(3.2,3,2.5,0), las=1)

#Particulate matter
plot(part.ts, type='n', xaxt='no', ylab='PM Conc.')
grid(lty=1, col=gray(.9))
lines(part.ts, col = "cadetblue3")

#Cardiovascular mortality
plot(cmort.ts, type='n', xaxt='no', ylab='Cardiac Mortality')
grid(lty=1, col=gray(.9))
lines(cmort.ts, col="orange3")

#Temperature
plot(temp.ts, type='n', ylab="Temperature")
grid(lty=1, col=gray(.9))
lines(temp.ts, col="forestgreen")

#Labels
mtext(side=1, "Year", line=2)
title(main = "Particulate Concentration, Cardiovascular Mortality, and Temperature", outer = T)
```

The first subplot (blue line) shows the weekly average particulate matter concentration. The next subplot (orange line) shows the weekly cardiovascular death count. The final line in green shows the average weekly temperature. From a descriptive stand point, a few things stand out about this plot: 

\item All of these lines exhibit a _periodicity_ or repetivite cycle of peaks and troughs.
\item The peaks in the particulate matter line (blue) and peaks in the cardiovascular mortality line (orange) are relatively similar -- they seem to occur within close proximity of each other.
\item The green line (temperature) appears to be somewhat offset from the cardiovascular and particulate matter lines. 


Down the line, we may eventually be interested in exploring the relationship (if any) between these time series. 

For now, we'll focus on one time series (cardiovascular mortality) and return to the question of multivariate time series later.

```{r, warning=F}
## Plotting correlation:
par(mfrow=c(3,3))
n <- seq(1,9,1)
for (i in 1:9){
  plot(as.vector(cmort),lag(as.vector(cmort),n=i),
       xlab= "C.Mort.", ylab=paste("Lag",i),
       main = paste("Lag",i))
}

## Plotting ACF
autoplot(acf(cmort.ts,plot=F, type = "correlation"))+labs(title="Correlogram of Cardiovascular Mortality, 1970-1980")+theme_bw()
```

# Exploratory data analysis for time series data in R

To measure dependence, we can use an **autocorrelation function (ACF)** in a time series model. ACF measures the linear predictability of the time series ($x_1,x_2,...x_n$) at time $s$ using only its adjacent value at time $t$ [@shumway_time_2006]. This can be done by using a linear combination of inputs. Thus, ACF provides the ability to forecast the series at time $s+1$ from the value of $x$ at time $t$ ($x_s$). 

What we also can do is plot the temperature data as another series to observe how particulate matter and cardiovascular mortality covary.

Note the strong seasonal components in all of the series, corresponding to winter-summer variations and the downward trend in the cardiovascular mortality over the 10-year period [@shumway_time_2006; @shumway_time_2017]. 


```{r}
#Correlation, scatter plots, and bivariate regression
df <- cbind(Particulates=part,
            Mortality=cmort,
            Temperature=tempr)
chart.Correlation(df)
```

Correlation matrix for the different time series. The diagonal displays the histogram and kernel density estimates of the variables. The lower off-diagonal shows the bivariate scatterplots with a lowess fitted line, and the upper off-diagonal shows the Pearson’s *R* coefficient with significance levels ($\cdot$ p < 0.1, ** p < 0.01, *** p < 0.001).


The scatterplots indicate a possible linear relation between mortality and the pollutant particulates, and between mortality and temperature. 


```{r}
plot(tempr, cmort, xlab="Temperature", ylab="Mortality")
lines(lowess(tempr, cmort))
```







# Modeling time series in R

```{r}
#Summary
summary(df)


#Center temperature
temp <- tempr-mean(tempr)
temp2 <- temp^2
trend <- time(cmort) # time

#Linear regression fit
summary(fit <- lm(cmort~ trend + temp + temp2 + part, na.action=NULL))

#ANOVA table (compare to next line)
summary(aov(fit)) 
summary(aov(lm(cmort~cbind(trend, temp, temp2, part)))) 


#AIC, BIC, AICc
num <- length(cmort) # sample size
AIC(fit)/num - log(2*pi) # AIC
BIC(fit)/num - log(2*pi) # BIC
(AICc <- log(sum(resid(fit)^2)/num) + (num+5)/(num-5-2)) #AiCc
```

We can relate mean adjusted temperature and particulate levels to cardiovascular mortality.



```{r}
#ACF
acf2(resid(fit), 52) # implies AR2
sarima(cmort, 2,0,0, xreg=cbind(trend,temp,temp2,part))
```

We assume that the current value at time *t* is white noise temporarily. The sample ACF and partial ACF (PACF) of
the residuals from the ordinary least squares fit of the model. The residual analysis output from `sarima` shows no obvious departure of the residuals from white noise.


## Time Series Analysis and Modeling with the Air Passengers Dataset

```{r}
library(ggfortify)
library(tseries)
library(forecast)
```

```{r}
data(AirPassengers)
AP <- AirPassengers
# Take a look at the class of the dataset AirPassengers
class(AP)
```

The AirPassenger dataset in R provides monthly totals of a US airline passengers, from 1949 to 1960. This dataset is already of a time series class therefore no further class or date manipulation is required.


### PERFORM EXPLORATORY DATA ANALYSIS

```{r}
# Take a look at the entries
AP
```


```{r}
# Check for missing values
sum(is.na(AP))
```

```{r}
# Check the frequency of the time series
frequency(AP)
```


```{r}
# Check the cycle of the time series
cycle(AP)
````

```{r}
# Plot the raw data using the base plot function
plot(AP,xlab="Date", ylab = "Passenger numbers (1000's)",main="Air Passenger numbers from 1949 to 1961")
```

As an alternative to the base plot function, so we can also use the extension ggfortify R package from the ggplot2 package, to plot directly from a time series. The benefits are not having to convert to a dataframe as required with ggplot2, but still having access to the layering grammar of graphics.

```{r}
autoplot(AP) + labs(x ="Date", y = "Passenger numbers (1000's)", title="Air Passengers from 1949 to 1961") 
```


```{r}
#Let’s use the boxplot function to see any seasonal effects.

boxplot(AP~cycle(AP),xlab="Date", ylab = "Passenger Numbers (1000's)" ,main ="Monthly Air Passengers Boxplot from 1949 to 1961")
```

From these exploratory plots, we can make some initial inferences:

The passenger numbers increase over time with each year which may be indicative of an increasing linear trend, perhaps due to increasing demand for flight travel and commercialisation of airlines in that time period.
In the boxplot there are more passengers travelling in months 6 to 9 with higher means and higher variances than the other months, indicating seasonality with a apparent cycle of 12 months. The rationale for this could be more people taking holidays and fly over the summer months in the US.
AirPassengers appears to be multiplicative time series as the passenger numbers increase, it appears so does the pattern of seasonality.
There do not appear to be any outliers and there are no missing values. Therefore no data cleaning is required.



### TIME SERIES DECOMPOSITION


```{r}
#With this model, we will use the decompose function in R. Continuing to use ggfortify for plots, in one line, autoplot these decomposed #components to further analyse the data.

decomposeAP <- decompose(AP,"multiplicative")
autoplot(decomposeAP)
```


In these decomposed plots we can again see the trend and seasonality as inferred previously, but we can also observe the estimation of the random component depicted under the “remainder”.


### TEST STATIONARITY OF THE TIME SERIES

A stationary time series has the conditions that the mean, variance and covariance are not functions of time. In order to fit arima models, the time series is required to be stationary. We will use two methods to test the stationarity.

1. Test stationarity of the time series (ADF)

In order to test the stationarity of the time series, let’s run the Augmented Dickey-Fuller Test using the adf.test function from the tseries R package.

First set the hypothesis test:

The null hypothesis H0
 : that the time series is non stationary
The alternative hypothesis HA
 : that the time series is stationary
 

```{r}
adf.test(AP) 
```

As a rule of thumb, where the p-value is less than 5%, we strong evidence against the null hypothesis, so we reject the null hypothesis. As per the test results above, the p-value is 0.01 which is <0.05 therefore we reject the null in favour of the alternative hypothesis that the time series is stationary.

2. Test stationarity of the time series (Autocorrelation)

Another way to test for stationarity is to use autocorrelation. We will use autocorrelation function (acf) in from the base stats R package. This function plots the correlation between a series and its lags ie previous observations with a 95% confidence interval in blue. If the autocorrelation crosses the dashed blue line, it means that specific lag is significantly correlated with current series.

```{r}
autoplot(acf(AP,plot=FALSE))+ labs(title="Correlogram of Air Passengers from 1949 to 1961") 
```


The maximum at lag 1 or 12 months, indicates a positive relationship with the 12 month cycle.

Since we have already created the decomposeAP list object with a random component, we can plot the acf of the decomposeAP$random.

```{r}
# Review random time series for any missing values
decomposeAP$random 
```

```{r}
# Autoplot the random time series from 7:138 which exclude the NA values
autoplot(acf(decomposeAP$random[7:138],plot=FALSE))+ labs(title="Correlogram of Air Passengers Random Component from 1949 to 1961") 
```

### FIT A TIME SERIES MODEL

1. Linear Model

Since there is an upwards trend we will look at a linear model first for comparison. We plot AirPassengers raw dataset with a blue linear model.

```{r}
autoplot(AP) + geom_smooth(method="lm")+ labs(x ="Date", y = "Passenger numbers (1000's)", title="Air Passengers from 1949 to 1961") 
```

This may not be the best model to fit as it doesn’t capture the seasonality and multiplicative effects over time.

2. ARIMA Model

Use the auto.arima function from the forecast R package to fit the best model and coefficients, given the default parameters including seasonality as TRUE. Note we have used the ARIMA modeling procedure as referenced

```{r}
arimaAP <- auto.arima(AP)
arimaAP
```

### CALCULATE FORECASTS

Finally we can plot a forecast of the time series using the forecast function, again from the forecast R package, with a 95% confidence interval where h is the forecast horizon periods in months.


```{r}

forecastAP <- forecast(arimaAP, level = c(95), h = 36)
autoplot(forecastAP)
```

# References










